---
title: "Stellar Classification"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

```

## Machine Learning Project - Academic year 2022/2023
Group members: Lorenzo Olearo, Alessandro Riva

```{r message=FALSE, warning=FALSE, paged.print=FALSE}

if (!require("FactoMineR")) install.packages("FactoMineR"); library("FactoMineR")
if (!require("factoextra")) install.packages("factoextra"); library("factoextra")
if (!require("e1071")) install.packages("e1071"); library("e1071")
if (!require("corrplot")) install.packages("corrplot"); library("corrplot")
if (!require("rpart")) install.packages("rpart"); library("rpart")
if (!require("rpart.plot")) install.packages("rpart.plot"); library("rpart.plot")
if (!require("caret")) install.packages("caret"); library("caret")

```

## Dataset Analysis
In astronomy, stellar classification is the classification of stars based on their spectral characteristics.
The classification scheme of galaxies, quasars, and stars is one of the most fundamental in astronomy.
The early cataloguing of stars and their distribution in the sky has led to the understanding that they make up our own galaxy and, following the distinction that Andromeda was a separate galaxy to our own, numerous galaxies began to be surveyed as more powerful telescopes were built.
This dataset aims at the classification of stars, galaxies, and quasars based on their spectral characteristics.

The data consists of 100,000 observations of space taken by the SDSS (Sloan Digital Sky Survey). 
Every observation is described by 17 feature columns and 1 class column which identifies it to be either a star, galaxy or quasar.


```{r}

dataset = read.csv("./dataset/star_classification.csv")
dim(dataset)

```

Let's look at the first 6 sample elements of the dataset

```{r}

head(dataset)

```

Now, we plot the different target class distribution for the dataset

```{r}

pie(table(dataset$class))

```

As shown in the pie chart above, the dataset contains 3 unbalanced classes, the first one the class of the galaxies with the 59% of the dataset, followed respectively by the class of the stars with 22% and the class of the quasar with 19% of the dataset.

### Dataset split

We proceed to split the dataset, the 70% of the dataset is used for the training and the 30% for the testing.
Note that the target column are separated from the two main set for further convenience.

```{r}

split.data = function(data, p = 0.7, s = 9000) {
  set.seed(s)
  index = sample(1:dim(data)[1])
  train = index[1:floor(dim(data)[1] * p)]
  test = index[((ceiling(dim(data)[1] * p)) + 1):dim(data)[1]]
  return(list(train=train, test=test)) 
}

dataset.target = dataset$class

split = split.data(dataset)
dataset.train = dataset[split$train, ]
dataset.test = dataset[split$test, ]
dataset.train.target = dataset.target[split$train]
dataset.test.target = dataset.target[split$test]

dim(dataset.train)
dim(dataset.test)

```

### Feature selection

We proceed to compute the correlation between the dataset various columns in order to select the most relevant features for the prediction of the target class.

```{r}

# Convert the dataset target column to numeric
dataset.train.target.numeric = as.numeric(factor(dataset.train.target))

dataset.train$rerun_ID <- NULL

dataset.train$class = as.numeric(factor(dataset.train$class))

# Compute the correlation matrix
correlation_on_target = cor(dataset.train, dataset.train.target.numeric, use = "pairwise.complete.obs")
print(correlation_on_target)

correlation_matrix = cor(dataset.train)
corrplot(correlation_matrix, method = 'color')

```

As shown in the correlation matrix, the columns u, g, r, i and z, corresponding the various spectral components measured by the SDSS, are correlated with each other. The PCA will later be used to reduce the dimensionality and redundancy in the dataset. 
Also deserving attention is the fact the multiple columns are identifiers for the specific observation contained in the row, those columns are dropped from the dataset.

Having computed and analyzed the correlation between the dataset various column and its target, the unnecessary columns are removed.
In order to keep the same number of columns in both the sets, the same columns are removed from the train set and the test set.

```{r}

dataset.train$obj_ID      <- NULL    # Object identifier
# dataset.train$alpha       <- NULL    # Right ascension angle
# dataset.train$delta       <- NULL    # Declination angle
dataset.train$run_ID      <- NULL    # Run number used to identify the specific scan
dataset.train$rerun_ID    <- NULL    # Rerun number that specify of the image was processed
dataset.train$cam_col     <- NULL    # Camera column to identify the scanline
dataset.train$field_ID    <- NULL    # Field number to identify each field
dataset.train$spec_obj_ID <- NULL    # Unique ID used for optical spectroscopic objects
dataset.train$plate       <- NULL    # Identifies each plate in SDSS
dataset.train$MJD         <- NULL    # Modified Julian date
dataset.train$fiber_ID    <- NULL    # Identifies the fiber that fired
dataset.train$class       <- NULL    # Target


# The same number of column is dropped from the test set in order to later apply the PCA transform computed on the train set
dataset.test$obj_ID      <- NULL    # Object identifier
# dataset.test$alpha       <- NULL    # Right ascension angle
# dataset.test$delta       <- NULL    # Declination angle
dataset.test$run_ID      <- NULL    # Run number used to identify the specific scan
dataset.test$rerun_ID    <- NULL    # Rerun number that specify of the image was processed
dataset.test$cam_col     <- NULL    # Camera column to identify the scanline
dataset.test$field_ID    <- NULL    # Field number to identify each field
dataset.test$spec_obj_ID <- NULL    # Unique ID used for optical spectroscopic objects
dataset.test$plate       <- NULL    # Identifies each plate in SDSS
dataset.test$MJD         <- NULL    # Modified Julian date
dataset.test$fiber_ID    <- NULL    # Identifies the fiber that fired
dataset.test$class       <- NULL    # Target

```

### Data normalization

The spectral components are highly correlated with each other, it therefore makes sense to normalize them with respect of each other.
The same applies for the delta and alpha columns, which are the right ascension and declination angles, respectively.
Lastly, the redshift column is normalized with respect of itself.
Note that both the train and the test dataset have to be normalized, however the normalization is done only with respect of the train dataset in order to avoid data leakage.

This normalization is done by scaling the values of each column between 0 and 1 and allows to achieve better results both on the PCA transform and the models built on top of it.

```{r}

scale_train = function(dataframe) {
  return ((dataframe - min(dataframe)) / (max(dataframe) - min(dataframe)))
}

scale_test = function(dataframe, maxi, mini) {
  return ((dataframe - mini) / (maxi - mini))
}


# The spectral components are normalized with respect of each other
subdataset.test = data.frame(
  u = dataset.test$u,
  r = dataset.test$r,
  i = dataset.test$i,
  g = dataset.test$g,
  z = dataset.test$z
)

subdataset.train = data.frame(
  u = dataset.train$u,
  r = dataset.train$r,
  i = dataset.train$i,
  g = dataset.train$g,
  z = dataset.train$z
)


subdataset.test = scale_test(subdataset.test, max(subdataset.train), min(subdataset.train))
subdataset.train = scale_train(subdataset.train)

dataset.test$u = subdataset.test[, 1]
dataset.test$r = subdataset.test[, 2]
dataset.test$i = subdataset.test[, 3]
dataset.test$g = subdataset.test[, 4]
dataset.test$z = subdataset.test[, 5]

dataset.train$u = subdataset.train[, 1]
dataset.train$r = subdataset.train[, 2]
dataset.train$i = subdataset.train[, 3]
dataset.train$g = subdataset.train[, 4]
dataset.train$z = subdataset.train[, 5]


# The position components are normalized with respect of each other
subdataset.test = data.frame(
  alpha = dataset.test$alpha,
  delta = dataset.test$delta
)

subdataset.train = data.frame(
  alpha = dataset.train$alpha,
  delta = dataset.train$delta
)

subdataset.test = scale_test(subdataset.test, max(subdataset.train), min(subdataset.train))
subdataset.train = scale_train(subdataset.train)

dataset.test$alpha = subdataset.test[, 1]
dataset.test$delta = subdataset.test[, 2]

dataset.train$alpha = subdataset.train[, 1]
dataset.train$delta = subdataset.train[, 2]


# redshift scaling
dataset.test$redshift = scale_test(dataset.test$redshift, max(dataset.train$redshift), min(dataset.train$redshift))
dataset.train$redshift = scale_train(dataset.train$redshift)

```

We compute the PCA transform to reduce the dimensionality of the dataset.
The corresponding eigenvalues are plotted to understand how many components are needed to explain the variance of the dataset. Furthermore, the correlation matrix of the PCA transformed dataset is computed and plotted to understand the correlation between the components.
Important to note that the PCA is being computed only on the train set in order to avoid data leakage.

```{r}

pca.train <- PCA(dataset.train, scale.unit = FALSE, ncp = 8, graph = FALSE)

# PCA Eigenvalues
eig <- get_eigenvalue(pca.train)
print('Eigenvalues:')
print(eig)
fviz_eig(pca.train, addlabels = TRUE, ylim = c(0, 50))
fviz_pca_var(pca.train, col.var = "black")

# Compute the correlation matrix on the PCA transformed train dataset
pca.train.cor = data.frame(get_pca_ind(pca.train)$coord)
pca.train.cor$class = dataset.train.target.numeric
pca.train.cor = cor(pca.train.cor)
corrplot(pca.train.cor, method = 'color')

```

As shown by the eigenvalues plot, the first three components explain more than 90% of the variance of the dataset, however, by analyzing the correlation matrix we can see that the fourth component is relatively highly correlated with the target class.
Due to this correlation, the fourth component is kept in the PCA transform despite the fact that explaining less than 4% of the variance.

Having achieved more than 95% of the variance with the first four components, we proced to apply the PCA transform over the train dataset.
Because the PCA transform is computed only on the train set, we need to apply the same transform over the test set in order to have the same dimensionality.

```{r}

pca.train = PCA(dataset.train, scale.unit = FALSE, ncp = 4, graph = FALSE)
pca.train.variables <- get_pca_var(pca.train)
pca.train.ind <- get_pca_ind(pca.train)
dim(pca.train.ind$coord)

dataset.train.pca = data.frame(pca.train.ind$coord)
dataset.train.pca$target <- factor(dataset.train.target)
dataset.train$target <- factor(dataset.train.target)

# Apply the PCA transform over the test dataset
pca.test = predict(pca.train, newdata=dataset.test)

dataset.test.pca = data.frame(pca.test$coord)
dataset.test.pca$target <- factor(dataset.test.target)
dataset.test$target <- factor(dataset.test.target)

```

## SVM

The first of the two models chosen is a Support Vector Machine (SVM) with a radial kernel.
The cost parameter is result of the grid search performed on the train set.
It being a relatively hight value (10) the model might be prone to overfitting.

```{r}

svm.radial = svm(
 target ~ .,
 data        = dataset.train.pca,
 kernel      = 'radial',
 probability = FALSE,
 cost        = 10
)

# svm.radial.tuned = tune.svm(target ~ Dim.1 + Dim.2 + Dim.3, data = dataset.train, kernel=c('radial'), cost=c(0.1, 1, 5, 10), probability=FALSE)

```

### Generate predictions

The test set is used to generate predictions and the confusion matrix is computed to evaluate the performance of the model.

```{r}

# Generate predictions
svm.pred = predict(svm.radial, dataset.test.pca)

# Create confusion matrix
confusionMatrix(data = svm.pred, reference = factor(dataset.test.target))

```

## Tree

A grid search is performed to identify the optimal complexity parameter value.

```{r}

# Grid search over complexity parameter values for the best fitting tree
tree.train.control = trainControl(method = "cv", number = 10)
tree.train.grid = expand.grid(cp = (1:100)*0.001)
tree.train = train(
  target ~ .,
  method    = "rpart",
  data      = dataset.train.pca,
  trControl = tree.train.control,
  tuneGrid  = tree.train.grid,
  metric    = "Kappa"
) 

tree = tree.train$finalModel

plot(tree.train)
prp(tree, type=0, extra = 1)

```

### Generate predictions

Again, the test set is used to generate predictions and the confusion matrix is computed to evaluate the performance of the model.

```{r}

# Generate predictions
tree.pred = predict(tree, dataset.test.pca, type="class")

# Create confusion matrix
confusionMatrix(data = tree.pred, reference = factor(dataset.test.target))

```


## Metrics



## Results
