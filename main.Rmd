---
title: "machine-learning-stars-prediction"
output:
  html_document: default
  pdf_document: default
date: "2023-01-08"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# install.packages("FactoMineR")
# install.packages("factoextra")
# install.packages("e1071")
```

```{r}
library("FactoMineR")
library("factoextra")
library("e1071")
```

## Dataset Analysis

The dataset is composed of 18 columns describing each of the 100000

```{r}
dataset = read.csv("./dataset/star_classification.csv")
dim(dataset)
```

Let's look at the first 5 sample elements of the dataset

```{r}
dataset[1:5,]
```

Now, we plot the different target class distribution for the dataset

```{r}
pie(table(dataset$class))
```

The unnecessary columns are removed

```{r}
dataset.target = dataset$class

dataset$obj_ID      <- NULL    # Object identifier
dataset$alfa        <- NULL    # Right ascension angle
dataset$delta       <- NULL    # Declination angle
dataset$run_ID      <- NULL    # Run number used to identify the specific scan
dataset$rerun_ID    <- NULL    # Rerun number that specify of the image was processed
dataset$cam_col     <- NULL    # Camera column to identify the scanline
dataset$field_ID    <- NULL    # Field number to identify each field
dataset$spec_obj_ID <- NULL    # Unique ID used for optical spectroscopic objects
dataset$plate       <- NULL    # Identifies each plate in SDSS
dataset$MJD         <- NULL    # Modified Julian date
dataset$fiber_ID    <- NULL    # Identifies the fiber that fired
dataset$class       <- NULL    # Target 
```

## Training and Test Datasets

```{r}
split.data = function(data, p = 0.7, s = 9000) {
  set.seed(s)
  index = sample(1:dim(data)[1])
  train = index[1:floor(dim(data)[1] * p)]
  test = index[((ceiling(dim(data)[1] * p)) + 1):dim(data)[1]]
  return(list(train=train, test=test)) 
}

split = split.data(dataset)
dataset.train = dataset[split$train, ]
dataset.test = dataset[split$test, ]
dataset.target.train = dataset.target[split$train]
dataset.target.test = dataset.target[split$test]

dim(dataset.train)
dim(dataset.test)
```

The PCA is computed and we can see that two dimensions explain more than 80% of the variance.

```{r}
dataset.train.pca <- PCA(dataset.train, scale.unit = TRUE, ncp = 7, graph = FALSE)

# PCA Eigenvalues
eig <- get_eigenvalue(dataset.train.pca)
print('Eigenvalues:')
print(eig)
fviz_eig(dataset.train.pca, addlabels = TRUE, ylim = c(0, 50))
fviz_pca_var(dataset.train.pca, col.var = "black")
```

Having achieved more than 80% explained variance with the first three dimensions we compute PCA again using only three dimensions and save the PCA variables

```{r}
dataset.train.pca = PCA(dataset.train, scale.unit = TRUE, ncp = 3, graph = FALSE)
dataset.train.pca.variables <- get_pca_var(dataset.train.pca)
dataset.train.pca.ind <- get_pca_ind(dataset.train.pca)
dim(dataset.train.pca.ind$coord)
```

## SVM

```{r}
dataset.train = data.frame(dataset.train.pca.ind$coord)
dataset.train$target <- factor(dataset.target.train)

plot(x=dataset.train[,1], y=dataset.train[,2], col = dataset.train$target)
plot(x=dataset.train[,1], y=dataset.train[,3], col = dataset.train$target)
plot(x=dataset.train[,2], y=dataset.train[,3], col = dataset.train$target)


#linear.svm = svm(target ~ Dim.1 + Dim.2 + Dim.3, data = dataset.train, kernel='radial', probability=TRUE)
#tuned.svm = tune.svm(target ~ Dim.1 + Dim.2 + Dim.3, data = dataset.train, kernel=c('radial'), cost=c(0.1, 1,5,10), probability=TRUE)
```

## Tree

## Neural Network

## Metrics

## Results

## Prediction on the test set

```{r}
dataset.test.pca <- predict(dataset.train.pca, newdata=dataset.test)

dataset.test.pca$coord[1:5, ]
dim(dataset.test.pca$coord)
```
