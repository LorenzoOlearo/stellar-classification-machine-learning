---
title: "machine-learning-stars-prediction"
output:
  html_document: default
  pdf_document: default
date: "2023-01-08"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE}
# install.packages("FactoMineR")
# install.packages("factoextra")
# install.packages("e1071")
# install.packages("corrplot")
# install.packages('caret')
# install.packages("DMwR2")
```

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
library("FactoMineR")
library("factoextra")
library("e1071")
library("corrplot")
library("caret")
library("DMwR2")
```

## Dataset Analysis

The dataset is composed of 18 columns describing each of the 100000

```{r}
dataset = read.csv("./dataset/star_classification.csv")
dim(dataset)
```

Let's look at the first 5 sample elements of the dataset

```{r}
dataset[1:5,]
```

Now, we plot the different target class distribution for the dataset

```{r}
pie(table(dataset$class))
```

### Dataset split

```{r}

split.data = function(data, p = 0.7, s = 9000) {
  set.seed(s)
  index = sample(1:dim(data)[1])
  train = index[1:floor(dim(data)[1] * p)]
  test = index[((ceiling(dim(data)[1] * p)) + 1):dim(data)[1]]
  return(list(train=train, test=test)) 
}

dataset.target = dataset$class

split = split.data(dataset)
dataset.train = dataset[split$train, ]
dataset.test = dataset[split$test, ]
dataset.train.target = dataset.target[split$train]
dataset.test.target = dataset.target[split$test]

dim(dataset.train)
dim(dataset.test)
```

### Feature selection

```{r}
# Convert the dataclass target column to numeric
dataset.train.target.numeric = as.numeric(factor(dataset.train.target))

dataset.train$class <- NULL
dataset.train$rerun_ID <- NULL

# Compute the correlation matrix
correlation_matrix = cor(dataset.train, dataset.train.target.numeric, use = "pairwise.complete.obs")

print(correlation_matrix)
```

Having computed the correlation between the dataset various column and its target, the unnecessary columns are removed

```{r}

dataset.train$obj_ID      <- NULL    # Object identifier
dataset.train$alpha       <- NULL    # Right ascension angle
# dataset.train$delta       <- NULL    # Declination angle
dataset.train$run_ID      <- NULL    # Run number used to identify the specific scan
dataset.train$rerun_ID    <- NULL    # Rerun number that specify of the image was processed
dataset.train$cam_col     <- NULL    # Camera column to identify the scanline
dataset.train$field_ID    <- NULL    # Field number to identify each field
# dataset.train$spec_obj_ID <- NULL    # Unique ID used for optical spectroscopic objects
# dataset.train$plate       <- NULL    # Identifies each plate in SDSS
# dataset.train$MJD         <- NULL    # Modified Julian date
dataset.train$fiber_ID    <- NULL    # Identifies the fiber that fired
dataset.train$class       <- NULL    # Target


# The same number of column is dropped from the test set in order to later apply the PCA transform computed on the train set
dataset.test$obj_ID      <- NULL    # Object identifier
dataset.test$alpha       <- NULL    # Right ascension angle
# dataset.test$delta       <- NULL    # Declination angle
dataset.test$run_ID      <- NULL    # Run number used to identify the specific scan
dataset.test$rerun_ID    <- NULL    # Rerun number that specify of the image was processed
dataset.test$cam_col     <- NULL    # Camera column to identify the scanline
dataset.test$field_ID    <- NULL    # Field number to identify each field
# dataset.test$spec_obj_ID <- NULL    # Unique ID used for optical spectroscopic objects
# dataset.test$plate       <- NULL    # Identifies each plate in SDSS
# dataset.test$MJD         <- NULL    # Modified Julian date
dataset.test$fiber_ID    <- NULL    # Identifies the fiber that fired
dataset.test$class       <- NULL    # Target
```

The PCA is computed and we can see that three dimensions explain more than 80% of the variance.

```{r}
dataset.train.pca <- PCA(dataset.train, scale.unit = TRUE, ncp = 7, graph = FALSE)

# PCA Eigenvalues
eig <- get_eigenvalue(dataset.train.pca)
print('Eigenvalues:')
print(eig)
fviz_eig(dataset.train.pca, addlabels = TRUE, ylim = c(0, 50))
fviz_pca_var(dataset.train.pca, col.var = "black")
```

Having achieved more than 80% explained variance with the first three dimensions we compute PCA again using only three dimensions and save the PCA variables

```{r}
dataset.train.pca = PCA(dataset.train, scale.unit = TRUE, ncp = 3, graph = FALSE)
dataset.train.pca.variables <- get_pca_var(dataset.train.pca)
dataset.train.pca.ind <- get_pca_ind(dataset.train.pca)
dim(dataset.train.pca.ind$coord)
```

## SVM

```{r}
dataset.train = data.frame(dataset.train.pca.ind$coord)
dataset.train$target <- factor(dataset.train.target)

svm.radial = svm(
  target ~ Dim.1 + Dim.2 + Dim.3,
  data        = dataset.train,
  kernel      = 'radial',
  probability = FALSE,
  cost        = 1
)


# radial.tuned.svm = tune.svm(target ~ Dim.1 + Dim.2 + Dim.3, data = dataset.train, kernel=c('radial'), cost=c(0.1, 1,5,10), probability=TRUE)
```

### Generate predictions

```{r}

# Apply the PCA transform over the test dataset
dataset.test.pca = predict(dataset.train.pca, newdata=dataset.test)
dataset.test.pca = data.frame(dataset.test.pca$coord)

dataset.test.pca$target = dataset.test.target
dataset.test.pca$target = factor(dataset.test.pca$target)


# Generate predictions
svm.pred = predict(svm.radial, dataset.test.pca)

# Create confusion matrix
svm.table = table(svm.pred, dataset.test.target)
svm.table
```

## Tree

## Neural Network

## Metrics

## Results

## Prediction on the test set

```{r}
dataset.test.pca <- predict(dataset.train.pca, newdata=dataset.test)

dataset.test.pca$coord[1:5, ]
dim(dataset.test.pca$coord)
```
