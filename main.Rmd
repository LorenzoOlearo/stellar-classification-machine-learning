---
title: "Stellar Classification"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

```

## Machine Learning Project - Academic year 2022/2023

Group members: Lorenzo Olearo, Alessandro Riva

```{r message=FALSE, warning=FALSE, paged.print=FALSE}

if (!require("FactoMineR")) install.packages("FactoMineR"); library("FactoMineR")
if (!require("factoextra")) install.packages("factoextra"); library("factoextra")
if (!require("e1071")) install.packages("e1071"); library("e1071")
if (!require("corrplot")) install.packages("corrplot"); library("corrplot")
if (!require("rpart")) install.packages("rpart"); library("rpart")
if (!require("rpart.plot")) install.packages("rpart.plot"); library("rpart.plot")
if (!require("caret")) install.packages("caret"); library("caret")
if (!require("randomForest")) install.packages("randomForest"); library("randomForest")
if (!require("tictoc")) install.packages("tictoc"); library("tictoc")
if (!require("ramify")) install.packages("ramify"); library("ramify")
if (!require("multiROC")) install.packages("multiROC"); library("multiROC")

```

## Dataset Analysis

In astronomy, stellar classification is the classification of stars based on their spectral characteristics. The classification scheme of galaxies, quasars, and stars is one of the most fundamental in astronomy. The early cataloguing of stars and their distribution in the sky has led to the understanding that they make up our own galaxy and, following the distinction that Andromeda was a separate galaxy to our own, numerous galaxies began to be surveyed as more powerful telescopes were built. This dataset aims at the classification of stars, galaxies, and quasars based on their spectral characteristics.

The data consists of 100,000 observations of space taken by the SDSS (Sloan Digital Sky Survey). Every observation is described by 17 feature columns and 1 class column which identifies it to be either a star, galaxy or quasar.

```{r}

dataset = read.csv("./dataset/star_classification.csv")
dim(dataset)

```

Let's look at the first 6 sample elements of the dataset

```{r}

head(dataset)

```

Now, we plot the different target class distribution for the dataset

```{r}

pie(table(dataset$class))

```

As shown in the pie chart above, the dataset contains 3 unbalanced classes, the first one is the class of the galaxies with the 59% of the dataset, followed respectively by the class of the stars with 22% and the class of the quasars with 19% of the dataset.

### Dataset split

We proceed to split the dataset, 70% of the dataset is used for the training and 30% for the testing. Note that the target columns are separated from the two main sets for further convenience.

```{r}

split.data = function(data, p = 0.7, s = 9000) {
  set.seed(s)
  index = sample(1:dim(data)[1])
  train = index[1:floor(dim(data)[1] * p)]
  test = index[((ceiling(dim(data)[1] * p)) + 1):dim(data)[1]]
  return(list(train=train, test=test)) 
}

dataset.target = factor(dataset$class)
dataset.target.prob = data.frame(
  STAR = as.numeric(dataset.target == "STAR"),
  GALAXY = as.numeric(dataset.target == "GALAXY"),
  QSO = as.numeric(dataset.target == "QSO")
)

split = split.data(dataset)
dataset.train = dataset[split$train, ]
dataset.test = dataset[split$test, ]
dataset.train.target = dataset.target[split$train]
dataset.test.target = dataset.target[split$test]
dataset.train.target.prob = dataset.target.prob[split$train,]
dataset.test.target.prob = dataset.target.prob[split$test,]

dim(dataset.train)
dim(dataset.test)

```

### Feature selection

The dataset is composed of 18 columns, 17 of which are features and 1 is the target class, the feature columns are the following:

1.  obj_ID = Object Identifier, the unique value that identifies the object in the image catalog used by the CAS
2.  alpha = Right Ascension angle (at J2000 epoch)
3.  delta = Declination angle (at J2000 epoch)
4.  u = Ultraviolet filter in the photometric system
5.  g = Green filter in the photometric system
6.  r = Red filter in the photometric system
7.  i = Near Infrared filter in the photometric system
8.  z = Infrared filter in the photometric system
9.  run_ID = Run Number used to identify the specific scan
10. rereun_ID = Rerun Number to specify how the image was processed
11. cam_col = Camera column to identify the scanline within the run
12. field_ID = Field number to identify each field
13. spec_obj_ID = Unique ID used for optical spectroscopic objects (this means that 2 different observations with the same spec_obj_ID must share the output class)
14. redshift = redshift value based on the increase in wavelength
15. plate = plate ID, identifies each plate in SDSS
16. MJD = Modified Julian Date, used to indicate when a given piece of SDSS data was taken
17. fiber_ID = fiber ID that identifies the fiber that pointed the light at the focal plane in each observation

We proceed to compute the correlation between the dataset various columns in order to select the most relevant features for the prediction of the target class.

```{r}

# Convert the dataset target column to numeric
dataset.train.target.numeric = as.numeric(factor(dataset.train.target))

dataset.train$rerun_ID <- NULL

dataset.train$class = as.numeric(factor(dataset.train$class))

# Compute the correlation matrix
correlation_on_target = cor(dataset.train, dataset.train.target.numeric, use = "pairwise.complete.obs")
print(correlation_on_target)

correlation_matrix = cor(dataset.train)
corrplot(correlation_matrix, method = 'color')

```

As shown in the correlation matrix, the columns u, g, r, i and z, corresponding to the various spectral components measured by the SDSS, are correlated with each other. The PCA will later be used to reduce the dimensionality and redundancy in the dataset. Also deserving attention is the fact that multiple columns are identifiers for the specific observation contained in the row, those columns are dropped from the dataset.

Having computed and analyzed the correlation between the dataset column and its target, the unnecessary columns are removed. In order to keep the same number of columns in both the sets, the same columns are removed from the train set and the test set.

```{r}

dataset.train$obj_ID      <- NULL    # Object identifier
dataset.train$run_ID      <- NULL    # Run number used to identify the specific scan
dataset.train$rerun_ID    <- NULL    # Rerun number that specify of the image was processed
dataset.train$cam_col     <- NULL    # Camera column to identify the scanline
dataset.train$field_ID    <- NULL    # Field number to identify each field
dataset.train$spec_obj_ID <- NULL    # Unique ID used for optical spectroscopic objects
dataset.train$plate       <- NULL    # Identifies each plate in SDSS
dataset.train$MJD         <- NULL    # Modified Julian date
dataset.train$fiber_ID    <- NULL    # Identifies the fiber that fired
dataset.train$class       <- NULL    # Target


# The same number of column is dropped from the test set in order to later apply the PCA transform computed on the train set
dataset.test$obj_ID      <- NULL    # Object identifier
dataset.test$run_ID      <- NULL    # Run number used to identify the specific scan
dataset.test$rerun_ID    <- NULL    # Rerun number that specify of the image was processed
dataset.test$cam_col     <- NULL    # Camera column to identify the scanline
dataset.test$field_ID    <- NULL    # Field number to identify each field
dataset.test$spec_obj_ID <- NULL    # Unique ID used for optical spectroscopic objects
dataset.test$plate       <- NULL    # Identifies each plate in SDSS
dataset.test$MJD         <- NULL    # Modified Julian date
dataset.test$fiber_ID    <- NULL    # Identifies the fiber that fired
dataset.test$class       <- NULL    # Target

```

### Data normalization

The spectral components are highly correlated with each other, it therefore makes sense to normalize them with respect of each other. The same applies for the delta and alpha columns, which are the right ascension and declination angles, respectively. Lastly, the redshift column is normalized with respect of itself. Note that both the train and the test dataset have to be normalized, however the normalization is done only with respect of the train dataset in order to avoid data leakage.

This normalization is done by scaling the values of each column between 0 and 1 and allows to achieve better results both on the PCA transform and the models built on top of it.

```{r}

scale_train = function(dataframe) {
  return ((dataframe - min(dataframe)) / (max(dataframe) - min(dataframe)))
}

scale_test = function(dataframe, maxi, mini) {
  return ((dataframe - mini) / (maxi - mini))
}


# The spectral components are normalized with respect of each other
subdataset.test = data.frame(
  u = dataset.test$u,
  r = dataset.test$r,
  i = dataset.test$i,
  g = dataset.test$g,
  z = dataset.test$z
)

subdataset.train = data.frame(
  u = dataset.train$u,
  r = dataset.train$r,
  i = dataset.train$i,
  g = dataset.train$g,
  z = dataset.train$z
)


subdataset.test = scale_test(subdataset.test, max(subdataset.train), min(subdataset.train))
subdataset.train = scale_train(subdataset.train)

dataset.test$u = subdataset.test[, 1]
dataset.test$r = subdataset.test[, 2]
dataset.test$i = subdataset.test[, 3]
dataset.test$g = subdataset.test[, 4]
dataset.test$z = subdataset.test[, 5]

dataset.train$u = subdataset.train[, 1]
dataset.train$r = subdataset.train[, 2]
dataset.train$i = subdataset.train[, 3]
dataset.train$g = subdataset.train[, 4]
dataset.train$z = subdataset.train[, 5]


# The position components are normalized with respect of each other
subdataset.test = data.frame(
  alpha = dataset.test$alpha,
  delta = dataset.test$delta
)

subdataset.train = data.frame(
  alpha = dataset.train$alpha,
  delta = dataset.train$delta
)

subdataset.test = scale_test(subdataset.test, max(subdataset.train), min(subdataset.train))
subdataset.train = scale_train(subdataset.train)

dataset.test$alpha = subdataset.test[, 1]
dataset.test$delta = subdataset.test[, 2]

dataset.train$alpha = subdataset.train[, 1]
dataset.train$delta = subdataset.train[, 2]


# redshift scaling
dataset.test$redshift = scale_test(dataset.test$redshift, max(dataset.train$redshift), min(dataset.train$redshift))
dataset.train$redshift = scale_train(dataset.train$redshift)

```

We compute the PCA transform to reduce the dimensionality of the dataset. The corresponding eigenvalues are plotted to understand how many components are needed to explain the variance of the dataset. Furthermore, the correlation matrix of the PCA transformed dataset is computed and plotted to understand the correlation between the components. Important to note that the PCA is being computed only on the train set in order to avoid data leakage.

```{r}

pca.train <- PCA(dataset.train, scale.unit = FALSE, ncp = 8, graph = FALSE)

# PCA Eigenvalues
eig <- get_eigenvalue(pca.train)
print('Eigenvalues:')
print(eig)
fviz_eig(pca.train, addlabels = TRUE)
fviz_pca_var(pca.train, col.var = "black")

# Compute the correlation matrix on the PCA transformed train dataset
pca.train.cor = data.frame(get_pca_ind(pca.train)$coord)
pca.train.cor$class = dataset.train.target.numeric
pca.train.cor = cor(pca.train.cor)
corrplot(pca.train.cor, method = 'color')

```

As shown by the eigenvalues plot, the first three components explain more than 90% of the variance of the dataset, however, by analyzing the correlation matrix we can see that the fourth component is relatively highly correlated with the target class. Due to this correlation, the fourth component is kept in the PCA transform despite it explaining less than 4% of the variance.

Having achieved more than 95% of the variance with the first four components, we proced to apply the PCA transform over the train dataset. Because the PCA transform is computed only on the train set, we need to apply the same transform over the test set in order to have the same dimensionality.

```{r}

pca.train = PCA(dataset.train, scale.unit = FALSE, ncp = 4, graph = FALSE)
pca.train.variables <- get_pca_var(pca.train)
pca.train.ind <- get_pca_ind(pca.train)
dim(pca.train.ind$coord)

dataset.train.pca = data.frame(pca.train.ind$coord)
dataset.train.pca$target <- factor(dataset.train.target)
dataset.train$target <- factor(dataset.train.target)

# Apply the PCA transform over the test dataset
pca.test = predict(pca.train, newdata=dataset.test)

dataset.test.pca = data.frame(pca.test$coord)
dataset.test.pca$target <- factor(dataset.test.target)
dataset.test$target <- factor(dataset.test.target)

```

## SVM

The first of the two models chosen is a Support Vector Machine (SVM) with a radial kernel. The cost parameter is result of the grid search performed on the train set. It being a relatively hight value (10) the model might be prone to overfitting.

```{r}

svm.radial = svm(
 target ~ .,
 data        = dataset.train.pca,
 kernel      = 'radial',
 probability = TRUE,
 cost        = 10
)

# svm.radial.tuned = tune.svm(target ~ Dim.1 + Dim.2 + Dim.3, data = dataset.train, kernel=c('radial'), cost=c(0.1, 1, 5, 10), probability=FALSE)

```

### Generate predictions

The test set is used to generate predictions and the confusion matrix is computed to evaluate the performance of the model.

```{r}

prob.to.factor = function(prob) {
  return(factor(colnames(prob)[argmax(prob)]))
}

# Generate predictions
tic()
svm.pred = predict(svm.radial, dataset.test.pca, type="prob", probability = TRUE)
svm.pred.time = toc()
svm.pred.time = (svm.pred.time$toc - svm.pred.time$tic)*10e+9/dim(dataset.test.pca)[1]
svm.pred = attr(svm.pred, "probabilities")

# Create confusion matrix
svm.conf.mat = confusionMatrix(data = prob.to.factor(svm.pred), reference = dataset.test.target)
print(svm.conf.mat)

```

## Decision Tree

The second model tested is a decision tree. The complexity parameter is select via grid search with cross validation. The best configuration is chosen based on the kappa statistic.

```{r}

# Grid search over complexity parameter values for the best fitting tree
tree.train.control = trainControl(method = "cv", number = 10)
tree.train.grid = expand.grid(cp = (1:100)*0.001)
tree.train = train(
  target ~ .,
  method    = "rpart",
  data      = dataset.train.pca,
  trControl = tree.train.control,
  tuneGrid  = tree.train.grid,
  metric    = "Kappa"
) 

tree = tree.train$finalModel

plot(tree.train)
prp(tree, type=0, extra = 1)

```

The plot shows how the kappa statistic worsen as the complexity parameter increases.

### Generate predictions

Again, the test set is used to generate predictions and the confusion matrix is computed to evaluate the performance of the model.

```{r}

# Generate predictions
tic()
tree.pred = predict(tree, dataset.test.pca, type="prob", probability = TRUE)
tree.pred.time = toc()
tree.pred.time = (tree.pred.time$toc - tree.pred.time$tic)*10e+9/dim(dataset.test.pca)[1]

# Create confusion matrix
tree.conf.mat = confusionMatrix(data = prob.to.factor(tree.pred), reference = dataset.test.target)
print(tree.conf.mat)

```

## Random Forest

Expanding on the idea of the decision tree, the random forest model is tested. The number of trees is optimized using a grid search.

```{r}

set.seed(108)

forest.tune = tune.randomForest(target ~ ., data = dataset.train.pca, ntree = c(1:15)*10)
forest = forest.tune$best.model

plot(forest.tune)

```

The plot shows how from 10 to 50 trees the error decreses remaining relatively stable after that.

### Generate predictions

The predictions on the test set are generated and the confusion matrix is computed to evaluate the performance of the model.

```{r}

# Generate predictions
tic()
forest.pred = predict(forest, dataset.test.pca, type="prob")
forest.pred.time = toc()
forest.pred.time = (forest.pred.time$toc - forest.pred.time$tic)*10e+9/dim(dataset.test.pca)[1]

# Create confusion matrix
forest.conf.mat = confusionMatrix(data = prob.to.factor(forest.pred), reference = dataset.test.target)
print(forest.conf.mat)

```

## Metrics

### Cohen's Kappa

The performances of the three models are evaluated using Cohen's Kappa, it being a better metric for a dataset with unbalanced classes.

The Cohen's Kappa is defined as follows:

$$
\kappa ={\frac {p_0-p_e}{1-p_e}}
$$

where

$$
p_0=\frac{ \sum_a e_{a,a}} {\sum_i \sum_j e_{i,j}}
$$

which is the accuracy between all classes, and

$$
p_e = \frac{\sum_a\big[ (\sum_i e_{i,a})(\sum_j e_{a,j})\big]}{\sum_i \sum_j e_{i,j}} 
$$

with $e_{i,j}$ the elements of the confusion matrix.

The kappa statistic has range [-1,+1] with 0 indicating a random classifier and 1 a perfect one, -1 indicates a totally wrong classification.

## Results

### ROC

```{r, message=FALSE, warning=FALSE}

roc.svm.data = data.frame(
  STAR_true = dataset.test.target.prob$STAR,
  QSO_true = dataset.test.target.prob$QSO,
  GALAXY_true = dataset.test.target.prob$GALAXY,
  STAR_pred_SVM = svm.pred[,"STAR"],
  QSO_pred_SVM = svm.pred[,"QSO"],
  GALAXY_pred_SVM = svm.pred[,"GALAXY"]
)

roc.tree.data = data.frame(
  STAR_true = dataset.test.target.prob$STAR,
  QSO_true = dataset.test.target.prob$QSO,
  GALAXY_true = dataset.test.target.prob$GALAXY,
  STAR_pred_TREE = tree.pred[,"STAR"],
  QSO_pred_TREE = tree.pred[,"QSO"],
  GALAXY_pred_TREE = tree.pred[,"GALAXY"]
)

roc.forest.data = data.frame(
  STAR_true = dataset.test.target.prob$STAR,
  QSO_true = dataset.test.target.prob$QSO,
  GALAXY_true = dataset.test.target.prob$GALAXY,
  STAR_pred_FOREST = forest.pred[,"STAR"],
  QSO_pred_FOREST = forest.pred[,"QSO"],
  GALAXY_pred_FOREST = forest.pred[,"GALAXY"]
)

roc.svm.res = multi_roc(roc.svm.data)
roc.tree.res = multi_roc(roc.tree.data)
roc.forest.res = multi_roc(roc.forest.data)

roc.svm.plot.data = plot_roc_data(roc.svm.res)
roc.tree.plot.data = plot_roc_data(roc.tree.res)
roc.forest.plot.data = plot_roc_data(roc.forest.res)

multi_roc_plot = function(data, resample = 1) {
  # multiROC plotting function
  # Original source at https://github.com/WandeRum/multiROC
  index = ((1:dim(data)[1]) %% resample) == 0
  data = data[index,]
  ggplot(data, aes(x = 1-Specificity, y=Sensitivity)) +
    geom_path(aes(color = Group, linetype=Method)) +
    geom_segment(aes(x = 0, y = 0, xend = 1, yend = 1), colour='grey', linetype = 'dotdash') +
    theme_bw() + 
    theme(
      plot.title = element_text(hjust = 0.5), 
      legend.justification=c(1, 0), legend.position=c(.95, .05),
      legend.title=element_blank(), 
      legend.background = element_rect(fill=NULL, linetype="solid", colour ="black")
    )
}

multi_roc_plot(roc.svm.plot.data, 100)
multi_roc_plot(roc.tree.plot.data, 100)
multi_roc_plot(roc.forest.plot.data, 100)

```

### Confusion matrices

```{r}

plot_conf_mat = function(table, plot.title) {
  plt <- as.data.frame(table)
  
  plt$Prediction <- factor(plt$Prediction, levels=rev(levels(plt$Prediction)))
  
  # ggplot original code from: https://stackoverflow.com/a/64539733  
  ggplot(plt, aes(Prediction,Reference, fill= Freq)) +
          geom_tile() + geom_text(aes(label=Freq)) +
          scale_fill_gradient(low="white", high="#85bdde") +
          labs(x = "Reference", y = "Prediction", title = plot.title) +
          scale_x_discrete(labels=c("GALAXY","QSO","STAR")) +
          scale_y_discrete(labels=c("STAR","QSO","GALAXY"))
}

svm.conf.mat.prob = svm.conf.mat
svm.conf.mat.prob$table[, 1] = round(svm.conf.mat.prob$table[, 1] / sum(svm.conf.mat.prob$table[, 1]) * 100, 2)
svm.conf.mat.prob$table[, 2] = round(svm.conf.mat.prob$table[, 2] / sum(svm.conf.mat.prob$table[, 2]) * 100, 2)
svm.conf.mat.prob$table[, 3] = round(svm.conf.mat.prob$table[, 3] / sum(svm.conf.mat.prob$table[, 3]) * 100, 2)

tree.conf.mat.prob = tree.conf.mat
tree.conf.mat.prob$table[, 1] = round(tree.conf.mat.prob$table[, 1] / sum(tree.conf.mat.prob$table[, 1]) * 100, 2)
tree.conf.mat.prob$table[, 2] = round(tree.conf.mat.prob$table[, 2] / sum(tree.conf.mat.prob$table[, 2]) * 100, 2)
tree.conf.mat.prob$table[, 3] = round(tree.conf.mat.prob$table[, 3] / sum(tree.conf.mat.prob$table[, 3]) * 100, 2)

forest.conf.mat.prob = forest.conf.mat
forest.conf.mat.prob$table[, 1] = round(forest.conf.mat.prob$table[, 1] / sum(forest.conf.mat.prob$table[, 1]) * 100, 2)
forest.conf.mat.prob$table[, 2] = round(forest.conf.mat.prob$table[, 2] / sum(forest.conf.mat.prob$table[, 2]) * 100, 2)
forest.conf.mat.prob$table[, 3] = round(forest.conf.mat.prob$table[, 3] / sum(forest.conf.mat.prob$table[, 3]) * 100, 2)

plot_conf_mat(svm.conf.mat.prob$table, plot.title = "SVM confusion matrix")
plot_conf_mat(tree.conf.mat.prob$table, plot.title = "Tree confusion matrix")
plot_conf_mat(forest.conf.mat.prob$table, plot.title = "Forest confusion matrix")

```

### Balanced Accuracy

```{r}

acc = data.frame(
  class  = rownames(svm.conf.mat$byClass),
  svm    = svm.conf.mat$byClass[, "Balanced Accuracy"],
  tree   = svm.conf.mat$byClass[, "Balanced Accuracy"],
  forest = svm.conf.mat$byClass[, "Balanced Accuracy"]
)

acc.svm = data.frame(
  class = colnames(svm.conf.mat$table),
  type = "svm",
  accuracy = svm.conf.mat$byClass[, "Balanced Accuracy"]
)

acc.tree = data.frame(
  class = colnames(tree.conf.mat$table),
  type = "tree",
  accuracy = tree.conf.mat$byClass[, "Balanced Accuracy"]
)

acc.forest = data.frame(
  class = colnames(forest.conf.mat$table),
  type = "forest",
  accuracy = forest.conf.mat$byClass[, "Balanced Accuracy"]
)

acc = rbind(acc.svm, acc.tree, acc.forest)

acc$balanced_accuracy <- round(acc$accuracy, 4)*100
acc$class <- factor(acc$class)
  
# ggplot original code from: https://stackoverflow.com/a/64539733  
ggplot(acc, aes(class, type, fill = balanced_accuracy)) +
        geom_tile() + geom_text(aes(label=balanced_accuracy)) +
        scale_fill_gradient(low="white", high="#85bdde") +
        labs(x = "Model",y = "Class", title = "Balanced Accuracy") +
        scale_x_discrete() +
        scale_y_discrete()

```

### Prediction time

Here the Cohen's Kappa values are shown in relation with the average prediction time.

```{r}

pred.comparison = data.frame(
  model = c("svm", "tree", "forest"), 
  time = c(svm = svm.pred.time, tree = tree.pred.time, forest = forest.pred.time)/1000, 
  kappa = c(svm = svm.conf.mat$overall["Kappa"], tree = tree.conf.mat$overall["Kappa"], forest = forest.conf.mat$overall["Kappa"])
)
print(pred.comparison)
plot(pred.comparison$time, pred.comparison$kappa, ylim = c(0.85, 0.95), ylab = "kappa", xlab = "average prediction time [ns]")
text(pred.comparison$time, pred.comparison$kappa-0.005, pred.comparison$model)

```

It is noticeable that while the SVM model produces a slightly better result than the random forest, the prediction time is much higher than the one of the other two models.
